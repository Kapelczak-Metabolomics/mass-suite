{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn import cluster,mixture\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.manifold import TSNE\n",
    "import scipy\n",
    "from pandas.core.common import flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea:\n",
    "blank removal --> noise removal --> find unique/shared clusters --> use information for id --> use information for tracking\n",
    "```\n",
    "sudo code:\n",
    "picking up sources\n",
    "output labeled table\n",
    "use the information for the source id\n",
    "\n",
    "two way:\n",
    "1. venn diagram --> source id\n",
    "2. more data: single source approach to identify clusters for different sources and use the modeling approach for source tracking\n",
    "```\n",
    "\n",
    "important : tweak the parameters during venn diagram approach\n",
    "\n",
    "Two ways: use source data + venn diagram, give unique cluster higher score and shared cluster lower score, given a new sample, can predict the source id, or assign with possibility scores, for instance >70% of cluster features present then the shource exists. Hard to do source tracking since matrix effect and dilution effect is not considered\n",
    "\n",
    "better way with more data: every source with dilution series, and use the single source approach to find clusters, and modeling for the source approportioning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ms = pd.read_csv('../example_data/clustering/sample1114.csv')\n",
    "d_ms = d_ms.rename(columns={'Average Rt(min)': 'Average RT (min)', 'Average Mz': 'Average m/z', 'S/N average': 'Average sn'})\n",
    "d_ms.insert(3, \"Average score\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(d_input, blank_keyword, simp_summary = False,svb_thres=10, empty_thres=0,rt_range=[0, 30], mz_range=[0, 1200], sn_thres=3, score_thres=0, area_thres=5000):\n",
    "    '''\n",
    "    The function is used to clean the dataframe according to user setting\n",
    "    blank_keyword: part of string from column that indicates the column is a blank sample\n",
    "    svb_thres: sample vs blank thres\n",
    "    empty_thres: empty cell thres in a row\n",
    "    cv_thres: as all sample is in triplicate, calculate the CV for every triplicate sample set #Needs to be updated in case there is no triplicate samples\n",
    "    rt_range: rt filter\n",
    "    mz_range: mz filter\n",
    "    sn_thres: signal/noise column thres\n",
    "    score_thres: score column thres\n",
    "    area_thres: count for max peak area from each row\n",
    "    '''\n",
    "    d_thres = d_input[d_input[d_input.columns[4:]].max(1) >= area_thres]\n",
    "    \n",
    "    d_thres = d_thres[(d_thres['Average RT (min)'] > rt_range[0]) & (d_thres['Average RT (min)'] < rt_range[1])]\n",
    "    d_thres = d_thres[(d_thres['Average m/z'] > mz_range[0]) & (d_thres['Average m/z'] < mz_range[1])]\n",
    "    d_thres = d_thres[d_thres['Average sn'] >= sn_thres]\n",
    "    d_thres = d_thres[d_thres['Average score'] >= score_thres]\n",
    "    d_thres.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    col_blank = []\n",
    "    for key in blank_keyword:\n",
    "        # Get column name if it contains blank indicating strings\n",
    "        col_blank.extend([col for col in d_thres.columns if key in col])\n",
    "        \n",
    "    col_sample = [col for col in d_thres.columns if col not in col_blank]\n",
    "    # Sample maximum area vs Blank average area to count for svb\n",
    "    d_sample = d_thres[d_thres[col_sample[4:]].max(axis=1) / d_thres[col_blank].mean(axis=1) > svb_thres][col_sample] \n",
    "    d_sample.reset_index(inplace=True)\n",
    "    d_sample.drop(columns=['index'],inplace=True)\n",
    "    \n",
    "    # Get a list of triplicate, every triplicate is in a sublist\n",
    "    #Sample: [[a1,a2,a3],[b1,b2,b3]]\n",
    "    #Note: the triplicate parsing is now only used '_' which needs update in the future\n",
    "    #d_transpose['dilu_vol'] = d_transpose['dilu_vol'].apply(lambda x : x.replace('-','_')) in case people use '-' for parsing\n",
    "    trip_list = [list(i) for j, i in groupby(d_sample.columns[4:], lambda a: a.split('_')[:-1])] \n",
    "    trip_list = [i for i in trip_list if len(i)>=2] #filter out columns that is not in triplicate -- sample naming issue\n",
    "\n",
    "    for triplicate in tqdm(trip_list):\n",
    "        for row in d_sample[triplicate].itertuples(): # Loop for every sets of triplicates\n",
    "            if row[1:].count(0) > empty_thres:\n",
    "                d_sample.loc[row.Index, triplicate] = 0 # if more than thres, then set all three values to 0\n",
    "#             elif np.mean(row[1:]) != 0:\n",
    "#                 if np.std(row[1:]) / np.mean(row[1:]) > cv_thres:\n",
    "#                     d_sample.loc[row.Index, triplicate] = 0 #need verify, not work for now\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "    d_sample = d_sample[~(d_sample[d_sample.columns[4:]]==0).all(1)] #clean rows with all 0\n",
    "    if simp_summary == True:\n",
    "        simp_dict={}\n",
    "        for i, column in enumerate(trip_list):\n",
    "            avg = d_sample[column].mean(1)\n",
    "            cv = d_sample[column].std(1) / d_sample[column].mean(1) #optional display CV\n",
    "            simp_dict.update({column[0][:-2]:avg, ' CV #' + str(i):cv})\n",
    "        d_result = pd.DataFrame(simp_dict)\n",
    "        d_result = pd.concat([d_sample[d_sample.columns[:4]], d_result], axis=1)\n",
    "    elif simp_summary == False:\n",
    "        d_result = d_sample.copy()\n",
    "    \n",
    "    return d_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [01:40<00:00,  2.87s/it]\n"
     ]
    }
   ],
   "source": [
    "keys=['CEC','Blank','ISTD','Wash','Shutdown']\n",
    "d_sample = data_prep(d_ms,keys,rt_range = [1,30], mz_range = [200,800], area_thres=500, simp_summary = False) # The function now only deal with triplicate samples\n",
    "#Needs to refine towards case that don't have 3 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. grouping\n",
    "2. noise_rm & filter\n",
    "3. source ID using avg PAs -- consider the score or other labels in the source label?\n",
    "4. calc dilution as below -- score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_label(d_input, sourcelist,area_thres=5000, concat = True): #noise removal only based on sourcelist cols\n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "    #source labeling\n",
    "    d_result = d_input.copy()\n",
    "    source_col=[]\n",
    "    for s in sourcelist:\n",
    "        source = [col for col in d_input.columns if s in col]\n",
    "        source_col.append(source)\n",
    "    simp_dict={}\n",
    "    for i, column in enumerate(source_col):\n",
    "        avg = d_result[column].mean(1)\n",
    "        cv = d_result[column].std(1) / d_result[column].mean(1) #optional display CV\n",
    "        cv_nan=np.isnan(cv)\n",
    "        cv[cv_nan]=0.0 #replace nan with 0\n",
    "        simp_dict.update({sourcelist[i]:avg, str(sourcelist[i])+' Cv':cv})\n",
    "    d_summary = pd.DataFrame(simp_dict)\n",
    "    d_summary['source']=\"NA\"\n",
    "    for row in d_summary.itertuples():\n",
    "        sourcelabel = list(d_summary.columns[[col_index for col_index, peak_avg in enumerate(row[1:-1]) if peak_avg >= area_thres]])\n",
    "        if len(sourcelabel) != 0:\n",
    "            labelstr = ','.join(sourcelabel)\n",
    "            d_summary.at[row.Index,'source'] = labelstr\n",
    "    if concat == True:\n",
    "        d_concat = pd.concat([d_result, d_summary], axis=1)\n",
    "    elif concat == False:\n",
    "        d_concat=d_result.copy()\n",
    "        d_concat['source'] = d_summary['source']\n",
    "    \n",
    "    return d_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourcelist=['Coulter','Crescent','Miller','Swan','SR520-Cal-in-DI_1000mL'] #Needs adjustment\n",
    "d_label = source_label(d_sample,sourcelist,area_thres= 50000,concat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User case for coverage score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# algorithm flow:\n",
    "\n",
    "1. simplify the chart to mean values, exclude CV variant ones\n",
    "2. using venn diagram idea, label the source according to mean peak area threshold\n",
    "3. get coverage score using the label information and get the intensity score similarly\n",
    "```\n",
    "but consider the matrix effect and unknowns to the sample, validation is required and more dedication & combination is needed\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_report(d_input, source_key, mix_key, method='multiple', pa_thres=10000, CV_thres=2): #source key needs to be the same as source_label above\n",
    "    #**only take the concat dataframe from labeling function\n",
    "    #prefilter & dataframe arrangement\n",
    "    d_mix = d_input[(d_input[[col for col in d_input.columns if 'Cv' in col]] <= CV_thres).all(1)]# all cv should below thres in order to be checked\n",
    "    d_simp = d_mix[source_key]\n",
    "    print('Threshold set to', thres)\n",
    "    mix_col = []\n",
    "    for key in mix_key:\n",
    "        mix_col.extend([col for col in d_mix.columns if 'Mix' in col])\n",
    "    if len(mix_col) == 0:\n",
    "        print(\"didn't find mixture by keyword!\")\n",
    "    source_col = []\n",
    "\n",
    "    d_st = pd.DataFrame(mix_col)\n",
    "\n",
    "    c_name = ['sample']\n",
    "    for source in source_key:\n",
    "        result = []\n",
    "        for col in mix_col:\n",
    "            n_feature = sum(d_mix[d_mix[col] >= thres]['source'].str.contains(source))\n",
    "            cov_score = n_feature / sum(d_mix['source'].str.contains(source))\n",
    "            if method == 'single':\n",
    "                mix = d_mix[d_mix['source'] == source][col]\n",
    "                s_simp = d_simp[d_simp['source'] == source][source]\n",
    "            elif method == 'multiple':\n",
    "                mix = d_mix[d_mix['source'].str.contains(source)][col]\n",
    "                s_simp = d_simp.loc[d_mix[d_mix['source'].str.contains(source)].index][source]\n",
    "            match_index = [i for i, j in enumerate(mix) if j >= thres]\n",
    "            dilu = mix.iloc[match_index] / s_simp.iloc[match_index]\n",
    "            ratio_score = np.average(dilu[dilu<1])\n",
    "            result.append([n_feature, cov_score, ratio_score])\n",
    "        d_st = pd.concat([d_st, pd.DataFrame(result)], axis = 1)\n",
    "        c_name.extend(['n_'+str(source), 'cover_s', 'ratio_s'])\n",
    "    d_st.columns = c_name\n",
    "    \n",
    "    return d_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold set to 5000\n"
     ]
    }
   ],
   "source": [
    "d_t = source_report(d_label, ['Coulter','Crescent','Miller','Swan','SR520-Cal-in-DI_1000mL'], ['Mix'], method='multiple', pa_thres=10000, CV_thres=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further comparison:\n",
    "\n",
    "ref Kathy's paper, compare the d_st with the cluster result/modeling report, further sort our features that is overlapping or meeting the criteria (table1 from the paper) to generate a detailed source tracking report use both source information and dilution information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualization based on single sample\n",
    "\n",
    "chart based on all \n",
    "\n",
    "|sample name|feature from source1|coverage score source1|etc2|etc2|\n",
    "|---|---|---|---|---|\n",
    "|sample1|1800|0.3|2000|0.5|\n",
    "|sample2|a|b|c|d|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first --> assign label to different features\n",
    "\n",
    "second --> ID using the features\n",
    "\n",
    "third --> assessment, for instance, final ID confidence = 50% feature quantity score + 50% feature intensity score\n",
    "\n",
    "sample A have 50% of source A feature, avg intensity ratio(5~95%) is 90%, then score = $0.5*0.5(feature #)+0.5*0.9$ (major feature intensity)\n",
    "\n",
    "for approportioning calculation --> matrix effect needs to be overcome --> more samples and data needed and will be a long term dev & validation process"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
