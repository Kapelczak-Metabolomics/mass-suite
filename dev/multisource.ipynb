{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn import cluster,mixture\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.manifold import TSNE\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea:\n",
    "blank removal --> noise removal --> find unique/shared clusters --> use information for id --> use information for tracking\n",
    "```\n",
    "sudo code:\n",
    "picking up sources\n",
    "output labeled table\n",
    "use the information for the source id\n",
    "\n",
    "two way:\n",
    "1. venn diagram --> source id\n",
    "2. more data: single source approach to identify clusters for different sources and use the modeling approach for source tracking\n",
    "```\n",
    "\n",
    "important : tweak the parameters during venn diagram approach\n",
    "\n",
    "Two ways: use source data + venn diagram, give unique cluster higher score and shared cluster lower score, given a new sample, can predict the source id, or assign with possibility scores, for instance >70% of cluster features present then the shource exists. Hard to do source tracking since matrix effect and dilution effect is not considered\n",
    "\n",
    "better way with more data: every source with dilution series, and use the single source approach to find clusters, and modeling for the source approportioning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ms = pd.read_csv('../example_data/clustering/sample1114.csv')\n",
    "d_ms = d_ms.rename(columns={'Average Rt(min)': 'Average RT (min)', 'Average Mz': 'Average m/z', 'S/N average': 'Average sn'})\n",
    "d_ms.insert(3, \"Average score\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(d_input, blank_keyword, svb_thres=10, empty_thres=0, cv_thres=5,rt_range=[0, 30], mz_range=[0, 1200], sn_thres=3, score_thres=0, area_thres=5000):\n",
    "    '''\n",
    "    The function is used to clean the dataframe according to user setting\n",
    "    blank_keyword: part of string from column that indicates the column is a blank sample\n",
    "    svb_thres: sample vs blank thres\n",
    "    empty_thres: empty cell thres in a row\n",
    "    cv_thres: as all sample is in triplicate, calculate the CV for every triplicate sample set #Needs to be updated in case there is no triplicate samples\n",
    "    rt_range: rt filter\n",
    "    mz_range: mz filter\n",
    "    sn_thres: signal/noise column thres\n",
    "    score_thres: score column thres\n",
    "    area_thres: count for max peak area from each row\n",
    "    '''\n",
    "    drop_index = np.argwhere(np.asarray(d_input[d_input.columns[4:]].max(axis=1)) < area_thres).reshape(1,-1) #Get the index for area thres filter\n",
    "    d_thres = d_input.drop(drop_index[0])\n",
    "    \n",
    "    d_thres = d_thres[(d_thres['Average RT (min)'] > rt_range[0]) & (d_thres['Average RT (min)'] < rt_range[1])]\n",
    "    d_thres = d_thres[(d_thres['Average m/z'] > mz_range[0]) & (d_thres['Average m/z'] < mz_range[1])]\n",
    "    d_thres = d_thres[d_thres['Average sn'] >= sn_thres]\n",
    "    d_thres = d_thres[d_thres['Average score'] >= score_thres]\n",
    "    d_thres.reset_index(inplace=True)\n",
    "    d_thres.drop(columns=['index'],inplace=True)\n",
    "    \n",
    "    col_blank = []\n",
    "    for key in blank_keyword:\n",
    "        col_app = [col for col in d_thres.columns if key in col] # Get column name if it contains blank indicating strings\n",
    "        col_blank += col_app\n",
    "    col_sample = [col for col in d_thres.columns if col not in col_blank]\n",
    "    \n",
    "    d_sample = d_thres[d_thres[col_sample[4:]].max(axis=1) / d_thres[col_blank].mean(axis=1) > svb_thres][col_sample] # Sample maximum area vs Blank average area to count for svb\n",
    "    d_sample.reset_index(inplace=True)\n",
    "    d_sample.drop(columns=['index'],inplace=True)\n",
    "    \n",
    "    # Get a list of triplicate, every triplicate is in a sublist\n",
    "    #Sample: [[a1,a2,a3],[b1,b2,b3]]\n",
    "    trip_list = [list(i) for j, i in groupby(d_sample.columns[4:], lambda a: a.split('_')[1])] #Note: the triplicate parsing is now only used '_' which needs update in the future\n",
    "\n",
    "    for triplicate in tqdm(trip_list):\n",
    "        for index, row in d_sample[triplicate].iterrows(): # Loop for every sets of triplicates\n",
    "            if (row == 0).sum() > empty_thres:\n",
    "                d_sample.loc[index, triplicate] = 0 # if more than thres, then set all three values to 0\n",
    "            elif row.std() / row.mean() > cv_thres:\n",
    "                d_sample.loc[index, triplicate] = 0 #If delete or reduce all number to avg?\n",
    "    #d_sample = d_sample[(d_sample.iloc[:,4:]!=0).sum(1) > 3]\n",
    "    \n",
    "    \n",
    "    return d_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:04<00:00, 12.43s/it]\n"
     ]
    }
   ],
   "source": [
    "keys=['CEC','Blank','ISTD','Wash','Shutdown']\n",
    "d_sample = data_prep(d_ms,keys,rt_range = [1,30], mz_range = [200,800], area_thres=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_detect(d_input, normalization='linear',eps=0.8,min_samples=10):\n",
    "    \n",
    "    c_data = d_input.iloc[:,4:].values\n",
    "    c_norm = []\n",
    "    #Performs normalization\n",
    "    for row in c_data:\n",
    "        if normalization == 'linear':\n",
    "            c_norm.append(row/max(row))\n",
    "        elif normalization == 'zscore':\n",
    "            c_norm.append((row-np.mean(row))/np.std(row))\n",
    "        elif normalization == 'log':\n",
    "            row[row==0]=1\n",
    "            c_norm.append(np.log10(row)/np.log10(max(row)))\n",
    "    #Clean up dataframe\n",
    "    c_norm = np.asarray(c_norm)\n",
    "    d_norm = pd.DataFrame(c_norm)\n",
    "    d_norm['index']=d_sample.index\n",
    "    d_norm.set_index('index',inplace=True)\n",
    "    d_norm.dropna(how='all',inplace=True)\n",
    "\n",
    "    X = d_norm.copy()\n",
    "    dbscan = cluster.DBSCAN(eps=eps, min_samples=min_samples).fit(X)\n",
    "    labels = dbscan.labels_\n",
    "    samp_index = np.argwhere(labels!=-1).reshape(1,-1)[0]\n",
    "    d_samp = d_input.iloc[samp_index]\n",
    "    \n",
    "    return d_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "d_f = noise_detect(d_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_label(d_input, sourcelist, area_thres=5000, method='any'):\n",
    "    source_col=[]\n",
    "    for s in sourcelist:\n",
    "        source = [col for col in d_input.columns if s in col]\n",
    "        source_col.append(source)\n",
    "    W\n",
    "    for index, row in d_input.iterrows():\n",
    "        sourcelabel=[]\n",
    "        for i, column in enumerate(source_col):\n",
    "            if method == 'any': #option of all\n",
    "                if (row[column] > area_thres).any() == True:\n",
    "                    sourcelabel.append(sourcelist[i])\n",
    "            elif method == 'max': #option of min\n",
    "                if row[column].max() > area_thres == True:\n",
    "                    sourcelabel.append(sourcelist[i])\n",
    "        if len(sourcelabel) != 0:\n",
    "            d_input.at[index,'source'] = sourcelabel\n",
    "    \n",
    "    return d_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourcelist=['Coulter','Crescent','SR520-Cal']\n",
    "d_test = source_label(d_f, sourcelist,method='any')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
