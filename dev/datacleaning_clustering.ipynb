{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn import cluster,mixture\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.manifold import TSNE\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ms = pd.read_csv('../example_data/clustering/sample1114.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ms = d_ms.rename(columns={'Average Rt(min)': 'Average RT (min)', 'Average Mz': 'Average m/z', 'S/N average': 'Average sn'})\n",
    "d_ms.insert(3, \"Average score\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys=['CEC','Blank','ISTD','Wash','Shutdown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Average RT (min)</th>\n",
       "      <th>Average m/z</th>\n",
       "      <th>Average sn</th>\n",
       "      <th>Average score</th>\n",
       "      <th>20181114_CEC_CAL-8-no4_MSpos_1</th>\n",
       "      <th>20181114_CEC_CAL-8-no4_MSpos_2</th>\n",
       "      <th>20181114_CEC_CAL-8-no4_MSpos_3</th>\n",
       "      <th>20181114_CEC_CAL-8-no4_MSpos_4</th>\n",
       "      <th>20181114_CEC_CAL-8-no4_MSpos_5</th>\n",
       "      <th>20181114_CEC_CAL-8-no4_MSpos_6</th>\n",
       "      <th>...</th>\n",
       "      <th>20181114_SR520-Creek_Mix6A_3</th>\n",
       "      <th>20181114_SR520-Creek_Mix6B_1</th>\n",
       "      <th>20181114_SR520-Creek_Mix6B_2</th>\n",
       "      <th>20181114_SR520-Creek_Mix6B_3</th>\n",
       "      <th>20181114_SwanCreek-Dec_1</th>\n",
       "      <th>20181114_SwanCreek-Dec_2</th>\n",
       "      <th>20181114_SwanCreek-Dec_3</th>\n",
       "      <th>20181114_SwanCreek-May_1</th>\n",
       "      <th>20181114_SwanCreek-May_2</th>\n",
       "      <th>20181114_SwanCreek-May_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.405</td>\n",
       "      <td>102.12830</td>\n",
       "      <td>328.62</td>\n",
       "      <td>1</td>\n",
       "      <td>6047</td>\n",
       "      <td>6216</td>\n",
       "      <td>7131</td>\n",
       "      <td>6745</td>\n",
       "      <td>6503</td>\n",
       "      <td>6536</td>\n",
       "      <td>...</td>\n",
       "      <td>18224</td>\n",
       "      <td>14682</td>\n",
       "      <td>22262</td>\n",
       "      <td>13669</td>\n",
       "      <td>2787</td>\n",
       "      <td>6175</td>\n",
       "      <td>1592</td>\n",
       "      <td>3837</td>\n",
       "      <td>2359</td>\n",
       "      <td>913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.669</td>\n",
       "      <td>104.10785</td>\n",
       "      <td>374.68</td>\n",
       "      <td>1</td>\n",
       "      <td>223176</td>\n",
       "      <td>226910</td>\n",
       "      <td>235522</td>\n",
       "      <td>236823</td>\n",
       "      <td>238360</td>\n",
       "      <td>233163</td>\n",
       "      <td>...</td>\n",
       "      <td>5605</td>\n",
       "      <td>11411</td>\n",
       "      <td>8035</td>\n",
       "      <td>8009</td>\n",
       "      <td>1951</td>\n",
       "      <td>1894</td>\n",
       "      <td>2223</td>\n",
       "      <td>6431</td>\n",
       "      <td>10595</td>\n",
       "      <td>5064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.644</td>\n",
       "      <td>104.99271</td>\n",
       "      <td>577.07</td>\n",
       "      <td>1</td>\n",
       "      <td>233200</td>\n",
       "      <td>229447</td>\n",
       "      <td>230362</td>\n",
       "      <td>198524</td>\n",
       "      <td>217569</td>\n",
       "      <td>189061</td>\n",
       "      <td>...</td>\n",
       "      <td>28642</td>\n",
       "      <td>226131</td>\n",
       "      <td>230926</td>\n",
       "      <td>225655</td>\n",
       "      <td>33866</td>\n",
       "      <td>35298</td>\n",
       "      <td>32832</td>\n",
       "      <td>43295</td>\n",
       "      <td>40695</td>\n",
       "      <td>41093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>16.068</td>\n",
       "      <td>117.11341</td>\n",
       "      <td>57.91</td>\n",
       "      <td>1</td>\n",
       "      <td>128170</td>\n",
       "      <td>160687</td>\n",
       "      <td>229006</td>\n",
       "      <td>181518</td>\n",
       "      <td>200239</td>\n",
       "      <td>132934</td>\n",
       "      <td>...</td>\n",
       "      <td>495954</td>\n",
       "      <td>521112</td>\n",
       "      <td>541949</td>\n",
       "      <td>540092</td>\n",
       "      <td>554232</td>\n",
       "      <td>582415</td>\n",
       "      <td>588641</td>\n",
       "      <td>447950</td>\n",
       "      <td>535279</td>\n",
       "      <td>545923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.683</td>\n",
       "      <td>118.08663</td>\n",
       "      <td>272.63</td>\n",
       "      <td>1</td>\n",
       "      <td>7370</td>\n",
       "      <td>7228</td>\n",
       "      <td>7960</td>\n",
       "      <td>7591</td>\n",
       "      <td>7546</td>\n",
       "      <td>7353</td>\n",
       "      <td>...</td>\n",
       "      <td>5653</td>\n",
       "      <td>4653</td>\n",
       "      <td>6328</td>\n",
       "      <td>5506</td>\n",
       "      <td>1159</td>\n",
       "      <td>1775</td>\n",
       "      <td>1521</td>\n",
       "      <td>2502</td>\n",
       "      <td>2710</td>\n",
       "      <td>2275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8477</th>\n",
       "      <td>19.490</td>\n",
       "      <td>1342.92346</td>\n",
       "      <td>235.35</td>\n",
       "      <td>1</td>\n",
       "      <td>83788</td>\n",
       "      <td>76357</td>\n",
       "      <td>74787</td>\n",
       "      <td>127416</td>\n",
       "      <td>72634</td>\n",
       "      <td>72594</td>\n",
       "      <td>...</td>\n",
       "      <td>297777</td>\n",
       "      <td>76221</td>\n",
       "      <td>75888</td>\n",
       "      <td>75134</td>\n",
       "      <td>135323</td>\n",
       "      <td>76623</td>\n",
       "      <td>275350</td>\n",
       "      <td>444276</td>\n",
       "      <td>301790</td>\n",
       "      <td>81517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8478</th>\n",
       "      <td>19.278</td>\n",
       "      <td>1342.92725</td>\n",
       "      <td>6974.32</td>\n",
       "      <td>1</td>\n",
       "      <td>3807801</td>\n",
       "      <td>3661432</td>\n",
       "      <td>3723328</td>\n",
       "      <td>3706440</td>\n",
       "      <td>3633766</td>\n",
       "      <td>3651315</td>\n",
       "      <td>...</td>\n",
       "      <td>3402720</td>\n",
       "      <td>3800249</td>\n",
       "      <td>3728474</td>\n",
       "      <td>3822507</td>\n",
       "      <td>3020332</td>\n",
       "      <td>3320772</td>\n",
       "      <td>3714521</td>\n",
       "      <td>3038139</td>\n",
       "      <td>4266428</td>\n",
       "      <td>2964256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8480</th>\n",
       "      <td>19.400</td>\n",
       "      <td>1343.93115</td>\n",
       "      <td>2477.24</td>\n",
       "      <td>1</td>\n",
       "      <td>135437</td>\n",
       "      <td>132070</td>\n",
       "      <td>303690</td>\n",
       "      <td>129267</td>\n",
       "      <td>141729</td>\n",
       "      <td>137975</td>\n",
       "      <td>...</td>\n",
       "      <td>136193</td>\n",
       "      <td>151433</td>\n",
       "      <td>294404</td>\n",
       "      <td>150204</td>\n",
       "      <td>142682</td>\n",
       "      <td>151517</td>\n",
       "      <td>1219615</td>\n",
       "      <td>166109</td>\n",
       "      <td>171770</td>\n",
       "      <td>169611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8481</th>\n",
       "      <td>19.279</td>\n",
       "      <td>1347.88513</td>\n",
       "      <td>2814.96</td>\n",
       "      <td>1</td>\n",
       "      <td>1291113</td>\n",
       "      <td>1271331</td>\n",
       "      <td>1324830</td>\n",
       "      <td>1285122</td>\n",
       "      <td>1317570</td>\n",
       "      <td>1330792</td>\n",
       "      <td>...</td>\n",
       "      <td>1203983</td>\n",
       "      <td>1257752</td>\n",
       "      <td>1186746</td>\n",
       "      <td>1476320</td>\n",
       "      <td>1543501</td>\n",
       "      <td>1368572</td>\n",
       "      <td>1518216</td>\n",
       "      <td>1081798</td>\n",
       "      <td>1583138</td>\n",
       "      <td>1603928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8482</th>\n",
       "      <td>19.272</td>\n",
       "      <td>1349.89038</td>\n",
       "      <td>1296.74</td>\n",
       "      <td>1</td>\n",
       "      <td>558219</td>\n",
       "      <td>551661</td>\n",
       "      <td>559269</td>\n",
       "      <td>550046</td>\n",
       "      <td>564333</td>\n",
       "      <td>486883</td>\n",
       "      <td>...</td>\n",
       "      <td>558883</td>\n",
       "      <td>547709</td>\n",
       "      <td>554203</td>\n",
       "      <td>635619</td>\n",
       "      <td>666030</td>\n",
       "      <td>626702</td>\n",
       "      <td>506495</td>\n",
       "      <td>654012</td>\n",
       "      <td>532172</td>\n",
       "      <td>598446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1526 rows × 158 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Average RT (min)  Average m/z  Average sn  Average score  \\\n",
       "9                3.405    102.12830      328.62              1   \n",
       "18               0.669    104.10785      374.68              1   \n",
       "22               0.644    104.99271      577.07              1   \n",
       "49              16.068    117.11341       57.91              1   \n",
       "52               0.683    118.08663      272.63              1   \n",
       "...                ...          ...         ...            ...   \n",
       "8477            19.490   1342.92346      235.35              1   \n",
       "8478            19.278   1342.92725     6974.32              1   \n",
       "8480            19.400   1343.93115     2477.24              1   \n",
       "8481            19.279   1347.88513     2814.96              1   \n",
       "8482            19.272   1349.89038     1296.74              1   \n",
       "\n",
       "      20181114_CEC_CAL-8-no4_MSpos_1  20181114_CEC_CAL-8-no4_MSpos_2  \\\n",
       "9                               6047                            6216   \n",
       "18                            223176                          226910   \n",
       "22                            233200                          229447   \n",
       "49                            128170                          160687   \n",
       "52                              7370                            7228   \n",
       "...                              ...                             ...   \n",
       "8477                           83788                           76357   \n",
       "8478                         3807801                         3661432   \n",
       "8480                          135437                          132070   \n",
       "8481                         1291113                         1271331   \n",
       "8482                          558219                          551661   \n",
       "\n",
       "      20181114_CEC_CAL-8-no4_MSpos_3  20181114_CEC_CAL-8-no4_MSpos_4  \\\n",
       "9                               7131                            6745   \n",
       "18                            235522                          236823   \n",
       "22                            230362                          198524   \n",
       "49                            229006                          181518   \n",
       "52                              7960                            7591   \n",
       "...                              ...                             ...   \n",
       "8477                           74787                          127416   \n",
       "8478                         3723328                         3706440   \n",
       "8480                          303690                          129267   \n",
       "8481                         1324830                         1285122   \n",
       "8482                          559269                          550046   \n",
       "\n",
       "      20181114_CEC_CAL-8-no4_MSpos_5  20181114_CEC_CAL-8-no4_MSpos_6  ...  \\\n",
       "9                               6503                            6536  ...   \n",
       "18                            238360                          233163  ...   \n",
       "22                            217569                          189061  ...   \n",
       "49                            200239                          132934  ...   \n",
       "52                              7546                            7353  ...   \n",
       "...                              ...                             ...  ...   \n",
       "8477                           72634                           72594  ...   \n",
       "8478                         3633766                         3651315  ...   \n",
       "8480                          141729                          137975  ...   \n",
       "8481                         1317570                         1330792  ...   \n",
       "8482                          564333                          486883  ...   \n",
       "\n",
       "      20181114_SR520-Creek_Mix6A_3  20181114_SR520-Creek_Mix6B_1  \\\n",
       "9                            18224                         14682   \n",
       "18                            5605                         11411   \n",
       "22                           28642                        226131   \n",
       "49                          495954                        521112   \n",
       "52                            5653                          4653   \n",
       "...                            ...                           ...   \n",
       "8477                        297777                         76221   \n",
       "8478                       3402720                       3800249   \n",
       "8480                        136193                        151433   \n",
       "8481                       1203983                       1257752   \n",
       "8482                        558883                        547709   \n",
       "\n",
       "      20181114_SR520-Creek_Mix6B_2  20181114_SR520-Creek_Mix6B_3  \\\n",
       "9                            22262                         13669   \n",
       "18                            8035                          8009   \n",
       "22                          230926                        225655   \n",
       "49                          541949                        540092   \n",
       "52                            6328                          5506   \n",
       "...                            ...                           ...   \n",
       "8477                         75888                         75134   \n",
       "8478                       3728474                       3822507   \n",
       "8480                        294404                        150204   \n",
       "8481                       1186746                       1476320   \n",
       "8482                        554203                        635619   \n",
       "\n",
       "      20181114_SwanCreek-Dec_1  20181114_SwanCreek-Dec_2  \\\n",
       "9                         2787                      6175   \n",
       "18                        1951                      1894   \n",
       "22                       33866                     35298   \n",
       "49                      554232                    582415   \n",
       "52                        1159                      1775   \n",
       "...                        ...                       ...   \n",
       "8477                    135323                     76623   \n",
       "8478                   3020332                   3320772   \n",
       "8480                    142682                    151517   \n",
       "8481                   1543501                   1368572   \n",
       "8482                    666030                    626702   \n",
       "\n",
       "      20181114_SwanCreek-Dec_3  20181114_SwanCreek-May_1  \\\n",
       "9                         1592                      3837   \n",
       "18                        2223                      6431   \n",
       "22                       32832                     43295   \n",
       "49                      588641                    447950   \n",
       "52                        1521                      2502   \n",
       "...                        ...                       ...   \n",
       "8477                    275350                    444276   \n",
       "8478                   3714521                   3038139   \n",
       "8480                   1219615                    166109   \n",
       "8481                   1518216                   1081798   \n",
       "8482                    506495                    654012   \n",
       "\n",
       "      20181114_SwanCreek-May_2  20181114_SwanCreek-May_3  \n",
       "9                         2359                       913  \n",
       "18                       10595                      5064  \n",
       "22                       40695                     41093  \n",
       "49                      535279                    545923  \n",
       "52                        2710                      2275  \n",
       "...                        ...                       ...  \n",
       "8477                    301790                     81517  \n",
       "8478                   4266428                   2964256  \n",
       "8480                    171770                    169611  \n",
       "8481                   1583138                   1603928  \n",
       "8482                    532172                    598446  \n",
       "\n",
       "[1526 rows x 158 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_ms[d_ms[d_ms.columns[4:]].max(1)>500000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(d_input, blank_keyword, svb_thres=10, empty_thres=0, cv_thres=5,rt_range=[0, 30], mz_range=[0, 1200], sn_thres=3, score_thres=0, area_thres=5000):\n",
    "    '''\n",
    "    The function is used to clean the dataframe according to user setting\n",
    "    blank_keyword: part of string from column that indicates the column is a blank sample\n",
    "    svb_thres: sample vs blank thres\n",
    "    empty_thres: empty cell thres in a row\n",
    "    cv_thres: as all sample is in triplicate, calculate the CV for every triplicate sample set #Needs to be updated in case there is no triplicate samples\n",
    "    rt_range: rt filter\n",
    "    mz_range: mz filter\n",
    "    sn_thres: signal/noise column thres\n",
    "    score_thres: score column thres\n",
    "    area_thres: count for max peak area from each row\n",
    "    '''\n",
    "    d_thres = d_input[d_input[d_input.columns[4:]].max(1) >= area_thres]\n",
    "    \n",
    "    d_thres = d_thres[(d_thres['Average RT (min)'] > rt_range[0]) & (d_thres['Average RT (min)'] < rt_range[1])]\n",
    "    d_thres = d_thres[(d_thres['Average m/z'] > mz_range[0]) & (d_thres['Average m/z'] < mz_range[1])]\n",
    "    d_thres = d_thres[d_thres['Average sn'] >= sn_thres]\n",
    "    d_thres = d_thres[d_thres['Average score'] >= score_thres]\n",
    "    d_thres.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    col_blank = []\n",
    "    for key in blank_keyword:\n",
    "        # Get column name if it contains blank indicating strings\n",
    "        col_blank.extend([col for col in d_thres.columns if key in col])\n",
    "        \n",
    "    col_sample = [col for col in d_thres.columns if col not in col_blank]\n",
    "    # Sample maximum area vs Blank average area to count for svb\n",
    "    d_sample = d_thres[d_thres[col_sample[4:]].max(axis=1) / d_thres[col_blank].mean(axis=1) > svb_thres][col_sample] \n",
    "    d_sample.reset_index(inplace=True)\n",
    "    d_sample.drop(columns=['index'],inplace=True)\n",
    "    \n",
    "    # Get a list of triplicate, every triplicate is in a sublist\n",
    "    #Sample: [[a1,a2,a3],[b1,b2,b3]]\n",
    "    #Note: the triplicate parsing is now only used '_' which needs update in the future\n",
    "    trip_list = [list(i) for j, i in groupby(d_sample.columns[4:], lambda a: a.split('_')[:-1])] \n",
    "    trip_list = [i for i in trip_list if len(i)>=2] #filter out columns that is not in triplicate -- sample naming issue\n",
    "\n",
    "    for triplicate in tqdm(trip_list):\n",
    "        # DM: maybe use iterrtuples? iterrows has low efficiency and is not reccomended \n",
    "        for row in d_sample[triplicate].itertuples(): # Loop for every sets of triplicates\n",
    "            if row[1:].count(0) > empty_thres:\n",
    "                d_sample.loc[row.Index, triplicate] = 0 # if more than thres, then set all three values to 0\n",
    "            elif np.mean(row[1:]) != 0:\n",
    "                if np.std(row[1:]) / np.mean(row[1:]) > cv_thres:\n",
    "                    d_sample.loc[row.Index, triplicate] = 0 #If delete or reduce all number to avg?\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "    d_sample = d_sample[~(d_sample[d_sample.columns[4:]]==0).all(1)] #clean rows with all 0\n",
    "    \n",
    "    return d_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [01:35<00:00,  2.73s/it]\n"
     ]
    }
   ],
   "source": [
    "d_sample = data_prep(d_ms,keys,rt_range = [1,30], mz_range = [200,800], area_thres=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ms_cluster(d_input, select_keyword, normalization='linear', visual=False, d_reduce=True, d_reduce_method='tsne', perplexity=20, cluster_method='dbscan',eps=0.8,min_samples=10):\n",
    "    '''\n",
    "    Function for direct clustering:\n",
    "    normalization method: linear, zscore, log\n",
    "    d_reduce: if perform the dimension reduction algorithm, method: only tsne is avilable now\n",
    "    perplexity: parameter for tsne\n",
    "    cluster_method: dbscan, later will update optic and spectrum\n",
    "    eps: parameter for dbscan, threshold of radius that used to count neighbours\n",
    "    min_samples: general parameter for clustering, min neighbourhoods to be counted as a cluster\n",
    "    '''\n",
    "    col_select = []\n",
    "    for key in select_keyword:\n",
    "        col_select.extend([col for col in d_input.columns if key in col])\n",
    "    d_clu = d_input[col_select]\n",
    "    \n",
    "    c_data = d_clu.values\n",
    "    c_norm = []\n",
    "    #Performs normalization\n",
    "    np.seterr(divide='ignore', invalid='ignore') #silent the warning -- but divide by 0 still exist\n",
    "    for row in c_data:\n",
    "        if normalization == 'linear':\n",
    "            c_norm.append(row/max(row))\n",
    "        elif normalization == 'zscore':\n",
    "            c_norm.append((row-np.mean(row))/np.std(row))\n",
    "        elif normalization == 'log':\n",
    "            row[row==0]=1\n",
    "            c_norm.append(np.log10(row)/np.log10(max(row)))\n",
    "        else:\n",
    "            pass\n",
    "    #Clean up dataframe\n",
    "    c_norm = np.asarray(c_norm)\n",
    "    d_norm = pd.DataFrame(c_norm)\n",
    "    d_norm['index']=d_sample.index\n",
    "    d_norm.set_index('index',inplace=True)\n",
    "    d_norm.dropna(how='all',inplace=True)\n",
    "    \n",
    "    if d_reduce == True:\n",
    "        if d_reduce_method == 'tsne':\n",
    "            model = TSNE(learning_rate=100,perplexity=50,n_iter=1000) #Tune perplexity and n_iter\n",
    "            transformed = model.fit_transform(d_norm)\n",
    "            d_feature = transformed.copy()\n",
    "        else:\n",
    "            pass\n",
    "    elif d_reduce == False:\n",
    "        d_feature = d_norm.copy()\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    if cluster_method == 'dbscan':\n",
    "        dbscan = cluster.DBSCAN(eps=eps, min_samples=min_samples).fit(d_feature)\n",
    "        labels = dbscan.labels_\n",
    "        unique_labels = set(dbscan.labels_)\n",
    "        \n",
    "        if visual == True:\n",
    "            for i,k in enumerate(unique_labels):\n",
    "                indexlist = list(np.argwhere(labels==k).reshape(1,-1)[0])\n",
    "                sns.clustermap(d_norm.iloc[indexlist].values,cmap='Reds',col_cluster=True,yticklabels=False,xticklabels=False,figsize=(5,5))\n",
    "                plt.title(str(dbscan)+'label='+ str(k))\n",
    "                plt.show()\n",
    "        else:\n",
    "            pass\n",
    "        d_init = d_sample.copy()\n",
    "        d_label = d_init.loc[d_norm.index] #Use the index to match back to the original datasheet\n",
    "        d_label.insert(4,\"label\", dbscan.labels_.tolist())\n",
    "    elif cluster_method == 'optics':\n",
    "        optics = cluster.OPTICS(min_samples=min_samples).fit(d_feature)\n",
    "        labels = optics.labels_\n",
    "        unique_labels = set(optics.labels_)\n",
    "        if visual == True:\n",
    "            for i,k in enumerate(unique_labels):\n",
    "                indexlist = list(np.argwhere(labels==k).reshape(1,-1)[0])\n",
    "                sns.clustermap(d_norm.iloc[indexlist].values,cmap='Reds',col_cluster=True,yticklabels=False,xticklabels=False,figsize=(5,5))\n",
    "                plt.title(str(optics)+'label='+ str(k))\n",
    "                plt.show()\n",
    "        else:\n",
    "            pass\n",
    "        d_init = d_sample.copy()\n",
    "        d_label = d_init.loc[d_norm.index] #Use the index to match back to the original datasheet\n",
    "        d_label.insert(4,\"label\", optics.labels_.tolist())\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    #Post filter -- filter out features that present in other sources but not SR520 -- keep it open for now\n",
    "    #If activate add one more variable:source_keyword\n",
    "#     col_source = []\n",
    "#     for key in source_keyword:\n",
    "#         col_app = [col for col in d_thres.columns if key in col]\n",
    "#         col_source += col_app\n",
    "#     col_rest = [col for col in d_label.columns if col not in source][5:]\n",
    "#     d_label[col_app].max(1) / d_label[col_rest].max(1)\n",
    "    \n",
    "    return d_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_label = ms_cluster(d_sample, ['SR520-Cal'], 'linear', d_reduce=False, visual=False, cluster_method='dbscan', eps=0.6, min_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options: all_data/clusters seperately\n",
    "# models: multiple linear/ random forest/ etc..\n",
    "# def modeling:\n",
    "#     select option\n",
    "#     select model\n",
    "#     if option all_data:\n",
    "#         model.fit(data) --> training 1114data, test 0815data\n",
    "#     elif option cluster:\n",
    "#         for group in cluster:\n",
    "#             model.fit(group)\n",
    "#         all_model -- > final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post filtering of dilution cluster\n",
    "# source tracking:\n",
    "#     1. samples from different sites\n",
    "#     2. vann diagram--> 'source subtraction' --> unique features for different source\n",
    "#     3. use cluster/noise distinguish method --> remove noises, get clusters\n",
    "#     4. source proportioning prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trend Clustering Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_calc(d_input, select_keyword, min_size=5, normalization='linear', method='pearsonr',visual=True):\n",
    "    \"\"\"This function calculates clustering based on the pearson correlation.\n",
    "    It takes in a dataframe and a user defined value for what qualifies as a cluster.\n",
    "    User can choose whether or not to have a visual plot of the scatter with True/False.\"\"\"\n",
    "    col_select = []\n",
    "    for key in select_keyword:\n",
    "        col_app = [col for col in d_input.columns if key in col]\n",
    "        col_select += col_app\n",
    "    d_clu = d_input[col_select]\n",
    "    \n",
    "    c_data = d_clu.values\n",
    "    c_norm = []\n",
    "    for row in c_data:\n",
    "        if normalization == 'linear':\n",
    "            c_norm.append(row/max(row))\n",
    "        elif normalization == 'zscore':\n",
    "            c_norm.append((row-np.mean(row))/np.std(row))\n",
    "        elif normalization == 'log':\n",
    "            row[row==0]=1\n",
    "            c_norm.append(np.log10(row)/np.log10(max(row)))\n",
    "    c_norm = np.asarray(c_norm)\n",
    "    d_norm = pd.DataFrame(c_norm)\n",
    "    d_norm['index']=d_sample.index\n",
    "    d_norm.set_index('index',inplace=True)\n",
    "    d_norm.dropna(how='all',inplace=True)\n",
    "    \n",
    "    #Post treatment to fit the d_norm into original codes\n",
    "    d_norm.insert(0,\"RT\", d_label['Average RT (min)'].tolist())\n",
    "    d_norm.insert(1,\"MZ\", d_label['Average m/z'].tolist())\n",
    "    d_norm = d_norm.reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    #Original codes\n",
    "    cluster = [] # individual cluster holder\n",
    "    cluster_sum = [] # total clusters\n",
    "    drop_list = [] # rows that are dropped from the df\n",
    "    noise = [] # list for containing noise features\n",
    "    while len(d_norm) > 0:\n",
    "        for row in range(len(d_norm)):\n",
    "            feature_1 = d_norm.iloc[0]\n",
    "            feature_2 = d_norm.iloc[row]\n",
    "            if method == 'pearsonr':\n",
    "                corr, p_val = scipy.stats.pearsonr(d_norm.iloc[0, 2:], d_norm.iloc[row, 2:]) \n",
    "            elif method == 'mannwhitneyu':\n",
    "                corr, p_val = scipy.stats.mannwhitneyu(d_norm.iloc[0, 2:], d_norm.iloc[row, 2:]) \n",
    "            elif method == 'kruskal':\n",
    "                corr, p_val = scipy.stats.kruskal(d_norm.iloc[0, 2:], d_norm.iloc[row, 2:]) \n",
    "            if p_val < 0.05:\n",
    "                drop_list.append(row)\n",
    "                cluster += [feature_2]\n",
    "            else:\n",
    "                pass\n",
    "        if len(cluster) <= min_size:\n",
    "            noise += [cluster]\n",
    "            cluster = []\n",
    "        else:\n",
    "            cluster_sum += [cluster]\n",
    "            cluster = []\n",
    "        d_norm = d_norm.drop(drop_list)\n",
    "        d_norm = d_norm.reset_index(drop=True)\n",
    "        drop_list = []\n",
    "    append_list = []\n",
    "    for i in range(len(cluster_sum)):\n",
    "        for j in range(len(cluster_sum[i])):\n",
    "            cluster_sum[i][j].loc['Score']= i\n",
    "            listing = np.array(cluster_sum[i][j])\n",
    "            append_list.append(listing)\n",
    "    cluster_df = pd.DataFrame(append_list) #Add columns use d_clu\n",
    "    append_list2 = []\n",
    "    for k in range(len(noise)):\n",
    "        for l in range(len(noise[k])):\n",
    "            noise[k][l].loc['Score']= -1\n",
    "            listing2 = np.array(noise[k][l])\n",
    "            append_list2.append(listing2)\n",
    "    noise_df = pd.DataFrame(append_list2)\n",
    "    final_df = pd.concat([cluster_df, noise_df])\n",
    "    final_df = final_df.reset_index(drop=True)\n",
    "    if visual == True:\n",
    "        labels = final_df.iloc[:,-1:].values.reshape(1,-1)[0]\n",
    "        unique_labels = set(labels)\n",
    "        for i,k in enumerate(unique_labels):\n",
    "            indexlist = list(np.argwhere(labels==k).reshape(1,-1)[0])\n",
    "            sns.clustermap(final_df.iloc[indexlist,2:-1].values,cmap='Reds',col_cluster=True,yticklabels=False,xticklabels=False,figsize=(5,5))\n",
    "            plt.title('trend'+'label='+ str(k))\n",
    "            plt.show()\n",
    "    else:\n",
    "        pass\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_1=trend_calc(d_sample, ['SR520-Cal'], min_size=5, normalization='zscore', visual=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test = pd.read_csv('../example_data/clustering/sample0815.csv')\n",
    "d_model = d_label[d_label['label']!=-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alignment of new dataset\n",
    "rt_error = 0.5\n",
    "mz_error = 0.015\n",
    "result = []\n",
    "for row in np.arange(len(d_model)):\n",
    "    overlap = np.where((d_test.iloc[:, 0] - rt_error <=\n",
    "                                    d_model.iloc[row, 0]) & (d_model.iloc[row, 0] <=\n",
    "                                    d_test.iloc[:, 0] + rt_error) & (d_test.iloc[:, 1] - mz_error <=\n",
    "                                    d_model.iloc[row, 1]) & (d_model.iloc[row, 1] <=\n",
    "                                    d_test.iloc[:, 1] + mz_error))\n",
    "    if len(overlap[0]) == 1:\n",
    "        result.append([overlap[0][0], row])\n",
    "    elif len(overlap[0]) > 1:\n",
    "        dist = []\n",
    "        for i in overlap[0]:\n",
    "            dist.append(np.sqrt(((d_test.iloc[i, 0] - d_model.iloc[row, 0])**2) +\n",
    "                                ((d_test.iloc[i, 1] - d_model.iloc[row, 1])**2)))\n",
    "        result.append([overlap[0][np.argmin(dist)], row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modeling using overlapping features except noises\n",
    "test_index = [i[0] for i in result]\n",
    "model_index = [i[1] for i in result]\n",
    "d_test = d_test.loc[test_index]\n",
    "d_model = d_model.iloc[model_index]\n",
    "data = [d_model, d_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_model = [col for col in d_model.columns if 'SR520-Cal' in col]\n",
    "d_model = d_model[col_model].T\n",
    "d_model.reset_index(inplace=True)\n",
    "d_model = d_model.rename(columns={'index':'dilu_vol'})\n",
    "d_model['dilu_vol'] = d_model['dilu_vol'].apply(lambda x : x.replace('-','_'))\n",
    "d_model['dilu_vol'] = d_model['dilu_vol'].apply(lambda x : float(x.split('_')[-2][:-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_test = [col for col in d_test.columns if 'SR520_Cal' in col]\n",
    "d_test = d_test[col_test].T\n",
    "d_test.reset_index(inplace=True)\n",
    "d_test = d_test.rename(columns={'index':'dilu_vol'})\n",
    "d_test['dilu_vol'] = d_test['dilu_vol'].apply(lambda x : float(x.split('_')[-2][:-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = d_model.iloc[: , 1:]\n",
    "y_train = d_model['dilu_vol']\n",
    "X_test = d_test.iloc[: , 1:]\n",
    "y_test = d_test['dilu_vol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7692307692307693\n"
     ]
    }
   ],
   "source": [
    "#Ref: selflearning/direct-modeling.ipynb\n",
    "#Due to small sample size, maybe consider decision tree or random forest rather than knn\n",
    "#Model all features at one step:\n",
    "model = RandomForestClassifier(n_estimators=100, \n",
    "                               bootstrap = True,\n",
    "                               max_features = 'sqrt')\n",
    "# Fit on training data\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred)) #How to deal with the data shape? -- align old data and new data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = linear_model.LinearRegression() #Colinearity is heavy -- grouping and reduce variable could impact alot\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1df1becd788>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(y_test,y_pred,marker='+')\n",
    "plt.plot([0,1000],[0,1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small data solution:\n",
    "1. use simple model\n",
    "2. emsemble method -- combine models from different clusters\n",
    "    1. for instance, seperately build model on n clusters and combine the prediction result at the end\n",
    "    2. further, do the iterative modeling -- take n features out from each cluster, build model, merge result, repeat the process\n",
    "3. use less features for modeling -- only use the features that is representative for the clusters\n",
    "\n",
    "this is especially useful in our case as most of the features within the same cluster should have strong collinearity\n",
    "    \n",
    "    strategy to reduce features in clusters:\n",
    "    0. before reduction, need to verify the cluster is correctly seperated\n",
    "    1. select features most close to the average (normalized average)\n",
    "    2. select features at lower/upper/25%/50%/75% boundaries\n",
    "    3. select features with highest intensity/lowest intensity\n",
    "    \n",
    "<font color = 'red'>4. get confidence interval rather than only a solid prediction result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
