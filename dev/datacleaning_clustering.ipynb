{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn import cluster,mixture\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.manifold import TSNE\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ms = pd.read_csv('../example_data/clustering/sample1114.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ms = d_ms.rename(columns={'Average Rt(min)': 'Average RT (min)', 'Average Mz': 'Average m/z', 'S/N average': 'Average sn'})\n",
    "d_ms.insert(3, \"Average score\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys=['CEC','Blank','ISTD','Wash','Shutdown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(d_input, blank_keyword, svb_thres=10, empty_thres=0, cv_thres=5,rt_range=[0, 30], mz_range=[0, 1200], sn_thres=3, score_thres=0, area_thres=5000):\n",
    "    '''\n",
    "    The function is used to clean the dataframe according to user setting\n",
    "    blank_keyword: part of string from column that indicates the column is a blank sample\n",
    "    svb_thres: sample vs blank thres\n",
    "    empty_thres: empty cell thres in a row\n",
    "    cv_thres: as all sample is in triplicate, calculate the CV for every triplicate sample set #Needs to be updated in case there is no triplicate samples\n",
    "    rt_range: rt filter\n",
    "    mz_range: mz filter\n",
    "    sn_thres: signal/noise column thres\n",
    "    score_thres: score column thres\n",
    "    area_thres: count for max peak area from each row\n",
    "    '''\n",
    "    #Get the index for area thres filter\n",
    "    # DM: can this be written simpler? Why do you need to drop the first index?\n",
    "    drop_index = np.argwhere(np.asarray(d_input[d_input.columns[4:]].max(axis=1)) < area_thres).reshape(1,-1) \n",
    "    d_thres = d_input.drop(drop_index[0])\n",
    "    \n",
    "    d_thres = d_thres[(d_thres['Average RT (min)'] > rt_range[0]) & (d_thres['Average RT (min)'] < rt_range[1])]\n",
    "    d_thres = d_thres[(d_thres['Average m/z'] > mz_range[0]) & (d_thres['Average m/z'] < mz_range[1])]\n",
    "    d_thres = d_thres[d_thres['Average sn'] >= sn_thres]\n",
    "    d_thres = d_thres[d_thres['Average score'] >= score_thres]\n",
    "    d_thres.reset_index(inplace=True)\n",
    "    d_thres.drop(columns=['index'],inplace=True)\n",
    "    \n",
    "    col_blank = []\n",
    "    for key in blank_keyword:\n",
    "        # Get column name if it contains blank indicating strings\n",
    "        # DM: can you just use col_blank without col_app?\n",
    "        col_app = [col for col in d_thres.columns if key in col] \n",
    "        col_blank += col_app\n",
    "    col_sample = [col for col in d_thres.columns if col not in col_blank]\n",
    "    # Sample maximum area vs Blank average area to count for svb\n",
    "    d_sample = d_thres[d_thres[col_sample[4:]].max(axis=1) / d_thres[col_blank].mean(axis=1) > svb_thres][col_sample] \n",
    "    d_sample.reset_index(inplace=True)\n",
    "    d_sample.drop(columns=['index'],inplace=True)\n",
    "    \n",
    "    # Get a list of triplicate, every triplicate is in a sublist\n",
    "    #Sample: [[a1,a2,a3],[b1,b2,b3]]\n",
    "    #Note: the triplicate parsing is now only used '_' which needs update in the future\n",
    "    trip_list = [list(i) for j, i in groupby(d_sample.columns[4:], lambda a: a.split('_')[1])] \n",
    "\n",
    "    for triplicate in tqdm(trip_list):\n",
    "        # DM: maybe use iterrtuples? iterrows has low efficiency and is not reccomended \n",
    "        for index, row in d_sample[triplicate].iterrows(): # Loop for every sets of triplicates\n",
    "            if (row == 0).sum() > empty_thres:\n",
    "                d_sample.loc[index, triplicate] = 0 # if more than thres, then set all three values to 0\n",
    "            elif row.std() / row.mean() > cv_thres:\n",
    "                d_sample.loc[index, triplicate] = 0 #If delete or reduce all number to avg?\n",
    "            else:\n",
    "                pass\n",
    "    #d_sample = d_sample[(d_sample.iloc[:,4:]!=0).sum(1) > 3]\n",
    "    \n",
    "    \n",
    "    return d_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:17<00:00, 13.77s/it]\n"
     ]
    }
   ],
   "source": [
    "d_sample = data_prep(d_ms,keys,rt_range = [1,30], mz_range = [200,800], area_thres=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ms_cluster(d_input, select_keyword, normalization='linear', visual=False, d_reduce=True, d_reduce_method='tsne', perplexity=20, cluster_method='dbscan',eps=0.8,min_samples=10):\n",
    "    '''\n",
    "    Function for direct clustering:\n",
    "    normalization method: linear, zscore, log\n",
    "    d_reduce: if perform the dimension reduction algorithm, method: only tsne is avilable now\n",
    "    perplexity: parameter for tsne\n",
    "    cluster_method: dbscan, later will update optic and spectrum\n",
    "    eps: parameter for dbscan, threshold of radius that used to count neighbours\n",
    "    min_samples: general parameter for clustering, min neighbourhoods to be counted as a cluster\n",
    "    '''\n",
    "    col_select = []\n",
    "    # DM: just use col_select instead of col_app?\n",
    "    for key in select_keyword:\n",
    "        col_app = [col for col in d_input.columns if key in col]\n",
    "        col_select += col_app\n",
    "    d_clu = d_input[col_select]\n",
    "    \n",
    "    c_data = d_clu.values\n",
    "    c_norm = []\n",
    "    #Performs normalization\n",
    "    for row in c_data:\n",
    "        if normalization == 'linear':\n",
    "            c_norm.append(row/max(row))\n",
    "        elif normalization == 'zscore':\n",
    "            c_norm.append((row-np.mean(row))/np.std(row))\n",
    "        elif normalization == 'log':\n",
    "            row[row==0]=1\n",
    "            c_norm.append(np.log10(row)/np.log10(max(row)))\n",
    "        else:\n",
    "            pass\n",
    "    #Clean up dataframe\n",
    "    c_norm = np.asarray(c_norm)\n",
    "    d_norm = pd.DataFrame(c_norm)\n",
    "    d_norm['index']=d_sample.index\n",
    "    d_norm.set_index('index',inplace=True)\n",
    "    d_norm.dropna(how='all',inplace=True)\n",
    "    \n",
    "    if d_reduce == True:\n",
    "        if d_reduce_method == 'tsne':\n",
    "            # DM: Maybe avoid using X as variable name?\n",
    "            model = TSNE(learning_rate=100,perplexity=50,n_iter=1000) #Tune perplexity and n_iter\n",
    "            transformed = model.fit_transform(d_norm)\n",
    "            X=transformed.copy()\n",
    "        else:\n",
    "            pass\n",
    "    elif d_reduce == False:\n",
    "        # DM: rename for clarity?\n",
    "        X=d_norm.copy()\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    if cluster_method == 'dbscan':\n",
    "        dbscan = cluster.DBSCAN(eps=eps, min_samples=min_samples).fit(X)\n",
    "        labels = dbscan.labels_\n",
    "        unique_labels = set(dbscan.labels_)\n",
    "        \n",
    "        if visual == True:\n",
    "            for i,k in enumerate(unique_labels):\n",
    "                indexlist = list(np.argwhere(labels==k).reshape(1,-1)[0])\n",
    "                sns.clustermap(d_norm.iloc[indexlist].values,cmap='Reds',col_cluster=True,yticklabels=False,xticklabels=False,figsize=(5,5))\n",
    "                plt.title(str(dbscan)+'label='+ str(k))\n",
    "                plt.show()\n",
    "        else:\n",
    "            pass\n",
    "        d_init = d_sample.copy()\n",
    "        d_label = d_init.loc[d_norm.index] #Use the index to match back to the original datasheet\n",
    "        d_label.insert(4,\"label\", dbscan.labels_.tolist())\n",
    "    elif cluster_method == 'optics':\n",
    "        optics = cluster.OPTICS(min_samples=min_samples).fit(X)\n",
    "        labels = optics.labels_\n",
    "        unique_labels = set(optics.labels_)\n",
    "        if visual == True:\n",
    "            for i,k in enumerate(unique_labels):\n",
    "                indexlist = list(np.argwhere(labels==k).reshape(1,-1)[0])\n",
    "                sns.clustermap(d_norm.iloc[indexlist].values,cmap='Reds',col_cluster=True,yticklabels=False,xticklabels=False,figsize=(5,5))\n",
    "                plt.title(str(optics)+'label='+ str(k))\n",
    "                plt.show()\n",
    "        else:\n",
    "            pass\n",
    "        d_init = d_sample.copy()\n",
    "        d_label = d_init.loc[d_norm.index] #Use the index to match back to the original datasheet\n",
    "        d_label.insert(4,\"label\", optics.labels_.tolist())\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    #Post filter -- filter out features that present in other sources but not SR520 -- keep it open for now\n",
    "    #If activate add one more variable:source_keyword\n",
    "#     col_source = []\n",
    "#     for key in source_keyword:\n",
    "#         col_app = [col for col in d_thres.columns if key in col]\n",
    "#         col_source += col_app\n",
    "#     col_rest = [col for col in d_label.columns if col not in source][5:]\n",
    "#     d_label[col_app].max(1) / d_label[col_rest].max(1)\n",
    "    \n",
    "    return d_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "d_label = ms_cluster(d_sample, ['SR520-Cal'], 'linear', d_reduce=False, visual=False, cluster_method='dbscan', eps=0.6, min_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options: all_data/clusters seperately\n",
    "# models: multiple linear/ random forest/ etc..\n",
    "# def modeling:\n",
    "#     select option\n",
    "#     select model\n",
    "#     if option all_data:\n",
    "#         model.fit(data) --> training 1114data, test 0815data\n",
    "#     elif option cluster:\n",
    "#         for group in cluster:\n",
    "#             model.fit(group)\n",
    "#         all_model -- > final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post filtering of dilution cluster\n",
    "# source tracking:\n",
    "#     1. samples from different sites\n",
    "#     2. vann diagram--> 'source subtraction' --> unique features for different source\n",
    "#     3. use cluster/noise distinguish method --> remove noises, get clusters\n",
    "#     4. source proportioning prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trend Clustering Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_calc(d_input, select_keyword, min_size=5, normalization='linear', visual=True):\n",
    "    \"\"\"This function calculates clustering based on the pearson correlation.\n",
    "    It takes in a dataframe and a user defined value for what qualifies as a cluster.\n",
    "    User can choose whether or not to have a visual plot of the scatter with True/False.\"\"\"\n",
    "    col_select = []\n",
    "    for key in select_keyword:\n",
    "        col_app = [col for col in d_input.columns if key in col]\n",
    "        col_select += col_app\n",
    "    d_clu = d_input[col_select]\n",
    "    \n",
    "    c_data = d_clu.values\n",
    "    c_norm = []\n",
    "    for row in c_data:\n",
    "        if normalization == 'linear':\n",
    "            c_norm.append(row/max(row))\n",
    "        elif normalization == 'zscore':\n",
    "            c_norm.append((row-np.mean(row))/np.std(row))\n",
    "        elif normalization == 'log':\n",
    "            row[row==0]=1\n",
    "            c_norm.append(np.log10(row)/np.log10(max(row)))\n",
    "    c_norm = np.asarray(c_norm)\n",
    "    d_norm = pd.DataFrame(c_norm)\n",
    "    d_norm['index']=d_sample.index\n",
    "    d_norm.set_index('index',inplace=True)\n",
    "    d_norm.dropna(how='all',inplace=True)\n",
    "    \n",
    "    #Post treatment to fit the d_norm into original codes\n",
    "    d_norm.insert(0,\"RT\", d_label['Average RT (min)'].tolist())\n",
    "    d_norm.insert(1,\"MZ\", d_label['Average m/z'].tolist())\n",
    "    d_norm = d_norm.reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    #Original codes\n",
    "    cluster = [] # individual cluster holder\n",
    "    cluster_sum = [] # total clusters\n",
    "    drop_list = [] # rows that are dropped from the df\n",
    "    noise = [] # list for containing noise features\n",
    "    while len(d_norm) > 0:\n",
    "        for row in range(len(d_norm)):\n",
    "            feature_1 = d_norm.iloc[0]\n",
    "            feature_2 = d_norm.iloc[row]\n",
    "            corr, p_val = scipy.stats.pearsonr(d_norm.iloc[0, 2:], d_norm.iloc[row, 2:]) #Potentially you can take the 2: off as d_norm.iloc[0] vs d_norm.iloc[row] \n",
    "            #And keep the index the same but not reset it, so you can use the index to link back to the d_input\n",
    "            if p_val < 0.05:\n",
    "                drop_list.append(row)\n",
    "                cluster += [feature_2]\n",
    "            else:\n",
    "                pass\n",
    "        if len(cluster) <= min_size:\n",
    "            noise += [cluster]\n",
    "            cluster = []\n",
    "        else:\n",
    "            cluster_sum += [cluster]\n",
    "            cluster = []\n",
    "        d_norm = d_norm.drop(drop_list)\n",
    "        d_norm = d_norm.reset_index(drop=True)\n",
    "        drop_list = []\n",
    "    append_list = []\n",
    "    for i in range(len(cluster_sum)):\n",
    "        for j in range(len(cluster_sum[i])):\n",
    "            cluster_sum[i][j].loc['Score']= i\n",
    "            listing = np.array(cluster_sum[i][j])\n",
    "            append_list.append(listing)\n",
    "    cluster_df = pd.DataFrame(append_list) #Add columns use d_clu\n",
    "    append_list2 = []\n",
    "    for k in range(len(noise)):\n",
    "        for l in range(len(noise[k])):\n",
    "            noise[k][l].loc['Score']= -1\n",
    "            listing2 = np.array(noise[k][l])\n",
    "            append_list2.append(listing2)\n",
    "    noise_df = pd.DataFrame(append_list2)\n",
    "    final_df = pd.concat([cluster_df, noise_df])\n",
    "    final_df = final_df.reset_index(drop=True)\n",
    "    if visual == True:\n",
    "        labels = final_df.iloc[:,-1:].values.reshape(1,-1)[0]\n",
    "        unique_labels = set(labels)\n",
    "        for i,k in enumerate(unique_labels):\n",
    "            indexlist = list(np.argwhere(labels==k).reshape(1,-1)[0])\n",
    "            sns.clustermap(final_df.iloc[indexlist,2:-1].values,cmap='Reds',col_cluster=True,yticklabels=False,xticklabels=False,figsize=(5,5))\n",
    "            plt.title('trend'+'label='+ str(k))\n",
    "            plt.show()\n",
    "    else:\n",
    "        pass\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d_1=trend_calc(d_sample, ['SR520-Cal'], min_size=5, normalization='zscore', visual=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test = pd.read_csv('../example_data/clustering/sample0815.csv')\n",
    "d_model = d_label[d_label['label']!=-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alignment of new dataset\n",
    "rt_error = 0.5\n",
    "mz_error = 0.015\n",
    "result = []\n",
    "for row in np.arange(len(d_model)):\n",
    "    overlap = np.where((d_test.iloc[:, 0] - rt_error <=\n",
    "                                    d_model.iloc[row, 0]) & (d_model.iloc[row, 0] <=\n",
    "                                    d_test.iloc[:, 0] + rt_error) & (d_test.iloc[:, 1] - mz_error <=\n",
    "                                    d_model.iloc[row, 1]) & (d_model.iloc[row, 1] <=\n",
    "                                    d_test.iloc[:, 1] + mz_error))\n",
    "    if len(overlap[0]) == 1:\n",
    "        result.append([overlap[0][0], row])\n",
    "    elif len(overlap[0]) > 1:\n",
    "        dist = []\n",
    "        for i in overlap[0]:\n",
    "            dist.append(np.sqrt(((d_test.iloc[i, 0] - d_model.iloc[row, 0])**2) +\n",
    "                                ((d_test.iloc[i, 1] - d_model.iloc[row, 1])**2)))\n",
    "        result.append([overlap[0][np.argmin(dist)], row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modeling using overlapping features except noises\n",
    "test_index = [i[0] for i in result]\n",
    "model_index = [i[1] for i in result]\n",
    "d_test = d_test.loc[test_index]\n",
    "d_model = d_model.iloc[model_index]\n",
    "data = [d_model, d_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_model = [col for col in d_model.columns if 'SR520-Cal' in col]\n",
    "d_model = d_model[col_model].T\n",
    "d_model.reset_index(inplace=True)\n",
    "d_model = d_model.rename(columns={'index':'dilu_vol'})\n",
    "d_model['dilu_vol'] = d_model['dilu_vol'].apply(lambda x : x.replace('-','_'))\n",
    "d_model['dilu_vol'] = d_model['dilu_vol'].apply(lambda x : float(x.split('_')[-2][:-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_test = [col for col in d_test.columns if 'SR520_Cal' in col]\n",
    "d_test = d_test[col_test].T\n",
    "d_test.reset_index(inplace=True)\n",
    "d_test = d_test.rename(columns={'index':'dilu_vol'})\n",
    "d_test['dilu_vol'] = d_test['dilu_vol'].apply(lambda x : float(x.split('_')[-2][:-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = d_model.iloc[: , 1:]\n",
    "y_train = d_model['dilu_vol']\n",
    "X_test = d_test.iloc[: , 1:]\n",
    "y_test = d_test['dilu_vol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7948717948717948\n"
     ]
    }
   ],
   "source": [
    "#Ref: selflearning/direct-modeling.ipynb\n",
    "#Due to small sample size, maybe consider decision tree or random forest rather than knn\n",
    "#Model all features at one step:\n",
    "model = RandomForestClassifier(n_estimators=100, \n",
    "                               bootstrap = True,\n",
    "                               max_features = 'sqrt')\n",
    "# Fit on training data\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred)) #How to deal with the data shape? -- align old data and new data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New function: to merge two data together from different batches -- similar to alignment but should be easier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
